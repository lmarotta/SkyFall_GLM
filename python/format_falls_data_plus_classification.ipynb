{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import scipy.io as sio\n",
    "import matlab.engine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "EngineError",
     "evalue": "Unable to connect to MATLAB session 'MATLAB_10948'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEngineError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-4c0e63484b91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# run matlab script to put data together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0meng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect_matlab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0meng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\ishparii\\dev\\SkyFall_GLM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnargout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0meng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnargout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ishparii\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matlab\\engine\\__init__.py\u001b[0m in \u001b[0;36mconnect_matlab\u001b[1;34m(name, async)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0masync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                 \u001b[0meng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0meng\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ishparii\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matlab\\engine\\futureresult.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TimeoutCannotBeNegative'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__future\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcancel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ishparii\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matlab\\engine\\matlabfuture.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     85\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LaunchMatlabTimeout'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpythonengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMATLAB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_future\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[0meng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatlabEngine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matlab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meng\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEngineError\u001b[0m: Unable to connect to MATLAB session 'MATLAB_10948'."
     ]
    }
   ],
   "source": [
    "# connect matlab engine to the existing matlab session\n",
    "eng = matlab.engine.connect_matlab()\n",
    "eng.cd(r'C:\\Users\\ishparii\\dev\\SkyFall_GLM', nargout=0)\n",
    "eng.ls(nargout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run matlab script to put data together\n",
    "data_CF = eng.TrainingDataSetup([],[],1,0) #location -use all; subjID - use all; n jitter - 1; condition - healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2602, 1215)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1205</th>\n",
       "      <th>1206</th>\n",
       "      <th>1207</th>\n",
       "      <th>1208</th>\n",
       "      <th>1209</th>\n",
       "      <th>1210</th>\n",
       "      <th>1211</th>\n",
       "      <th>1212</th>\n",
       "      <th>1213</th>\n",
       "      <th>1214</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.125901</td>\n",
       "      <td>0.077848</td>\n",
       "      <td>-0.112097</td>\n",
       "      <td>-0.011347</td>\n",
       "      <td>0.026207</td>\n",
       "      <td>0.056144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827352</td>\n",
       "      <td>-2.067576</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.727249</td>\n",
       "      <td>0.049420</td>\n",
       "      <td>0.135441</td>\n",
       "      <td>0.055171</td>\n",
       "      <td>0.030166</td>\n",
       "      <td>0.091560</td>\n",
       "      <td>0.060249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.033489</td>\n",
       "      <td>0.214405</td>\n",
       "      <td>-0.042801</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.163193</td>\n",
       "      <td>0.102776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959836</td>\n",
       "      <td>-1.320655</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.544320</td>\n",
       "      <td>0.040074</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.064495</td>\n",
       "      <td>0.057668</td>\n",
       "      <td>0.040334</td>\n",
       "      <td>0.029518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.078853</td>\n",
       "      <td>0.225161</td>\n",
       "      <td>-0.003258</td>\n",
       "      <td>0.106326</td>\n",
       "      <td>0.214892</td>\n",
       "      <td>0.254953</td>\n",
       "      <td>...</td>\n",
       "      <td>1.985645</td>\n",
       "      <td>3.946246</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.082254</td>\n",
       "      <td>0.040935</td>\n",
       "      <td>0.116699</td>\n",
       "      <td>0.070139</td>\n",
       "      <td>0.065564</td>\n",
       "      <td>0.061595</td>\n",
       "      <td>0.071664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.427013</td>\n",
       "      <td>0.124016</td>\n",
       "      <td>0.209373</td>\n",
       "      <td>-0.285074</td>\n",
       "      <td>0.104775</td>\n",
       "      <td>-0.032565</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156472</td>\n",
       "      <td>4.686816</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.393702</td>\n",
       "      <td>0.034751</td>\n",
       "      <td>0.106485</td>\n",
       "      <td>0.036636</td>\n",
       "      <td>0.047692</td>\n",
       "      <td>0.069517</td>\n",
       "      <td>0.017662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.258621</td>\n",
       "      <td>0.147098</td>\n",
       "      <td>0.127442</td>\n",
       "      <td>-0.136984</td>\n",
       "      <td>0.028601</td>\n",
       "      <td>-0.016087</td>\n",
       "      <td>...</td>\n",
       "      <td>2.010624</td>\n",
       "      <td>4.058232</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.253397</td>\n",
       "      <td>0.035888</td>\n",
       "      <td>0.147006</td>\n",
       "      <td>0.043686</td>\n",
       "      <td>0.049197</td>\n",
       "      <td>0.026301</td>\n",
       "      <td>0.069618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3         4         5         6         7         8     \\\n",
       "0   1.0   1.0   1.0   3.0  0.125901  0.077848 -0.112097 -0.011347  0.026207   \n",
       "1   1.0   1.0   1.0   3.0 -0.033489  0.214405 -0.042801  0.036700  0.163193   \n",
       "2   1.0   1.0   1.0   3.0  0.078853  0.225161 -0.003258  0.106326  0.214892   \n",
       "3   1.0   1.0   1.0   4.0 -0.427013  0.124016  0.209373 -0.285074  0.104775   \n",
       "4   1.0   1.0   1.0   4.0 -0.258621  0.147098  0.127442 -0.136984  0.028601   \n",
       "\n",
       "       9       ...         1205      1206      1207      1208      1209  \\\n",
       "0  0.056144    ...     0.827352 -2.067576  1.370951  0.727249  0.049420   \n",
       "1  0.102776    ...     0.959836 -1.320655  1.370951  0.544320  0.040074   \n",
       "2  0.254953    ...     1.985645  3.946246  1.370951  0.082254  0.040935   \n",
       "3 -0.032565    ...     2.156472  4.686816  1.370951  0.393702  0.034751   \n",
       "4 -0.016087    ...     2.010624  4.058232  1.370951  0.253397  0.035888   \n",
       "\n",
       "       1210      1211      1212      1213      1214  \n",
       "0  0.135441  0.055171  0.030166  0.091560  0.060249  \n",
       "1  0.131500  0.064495  0.057668  0.040334  0.029518  \n",
       "2  0.116699  0.070139  0.065564  0.061595  0.071664  \n",
       "3  0.106485  0.036636  0.047692  0.069517  0.017662  \n",
       "4  0.147006  0.043686  0.049197  0.026301  0.069618  \n",
       "\n",
       "[5 rows x 1215 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_CF_np = np.array(data_CF)\n",
    "data_CF_df = pd.DataFrame(data_CF_np)\n",
    "\n",
    "print(data_CF_df.shape)\n",
    "data_CF_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj_id</th>\n",
       "      <th>location</th>\n",
       "      <th>subj_code</th>\n",
       "      <th>label</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1205</th>\n",
       "      <th>1206</th>\n",
       "      <th>1207</th>\n",
       "      <th>1208</th>\n",
       "      <th>1209</th>\n",
       "      <th>1210</th>\n",
       "      <th>1211</th>\n",
       "      <th>1212</th>\n",
       "      <th>1213</th>\n",
       "      <th>1214</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.125901</td>\n",
       "      <td>0.077848</td>\n",
       "      <td>-0.112097</td>\n",
       "      <td>-0.011347</td>\n",
       "      <td>0.026207</td>\n",
       "      <td>0.056144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827352</td>\n",
       "      <td>-2.067576</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.727249</td>\n",
       "      <td>0.049420</td>\n",
       "      <td>0.135441</td>\n",
       "      <td>0.055171</td>\n",
       "      <td>0.030166</td>\n",
       "      <td>0.091560</td>\n",
       "      <td>0.060249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.033489</td>\n",
       "      <td>0.214405</td>\n",
       "      <td>-0.042801</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.163193</td>\n",
       "      <td>0.102776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959836</td>\n",
       "      <td>-1.320655</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.544320</td>\n",
       "      <td>0.040074</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.064495</td>\n",
       "      <td>0.057668</td>\n",
       "      <td>0.040334</td>\n",
       "      <td>0.029518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.078853</td>\n",
       "      <td>0.225161</td>\n",
       "      <td>-0.003258</td>\n",
       "      <td>0.106326</td>\n",
       "      <td>0.214892</td>\n",
       "      <td>0.254953</td>\n",
       "      <td>...</td>\n",
       "      <td>1.985645</td>\n",
       "      <td>3.946246</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.082254</td>\n",
       "      <td>0.040935</td>\n",
       "      <td>0.116699</td>\n",
       "      <td>0.070139</td>\n",
       "      <td>0.065564</td>\n",
       "      <td>0.061595</td>\n",
       "      <td>0.071664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.427013</td>\n",
       "      <td>0.124016</td>\n",
       "      <td>0.209373</td>\n",
       "      <td>-0.285074</td>\n",
       "      <td>0.104775</td>\n",
       "      <td>-0.032565</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156472</td>\n",
       "      <td>4.686816</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.393702</td>\n",
       "      <td>0.034751</td>\n",
       "      <td>0.106485</td>\n",
       "      <td>0.036636</td>\n",
       "      <td>0.047692</td>\n",
       "      <td>0.069517</td>\n",
       "      <td>0.017662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.258621</td>\n",
       "      <td>0.147098</td>\n",
       "      <td>0.127442</td>\n",
       "      <td>-0.136984</td>\n",
       "      <td>0.028601</td>\n",
       "      <td>-0.016087</td>\n",
       "      <td>...</td>\n",
       "      <td>2.010624</td>\n",
       "      <td>4.058232</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.253397</td>\n",
       "      <td>0.035888</td>\n",
       "      <td>0.147006</td>\n",
       "      <td>0.043686</td>\n",
       "      <td>0.049197</td>\n",
       "      <td>0.026301</td>\n",
       "      <td>0.069618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subj_id  location  subj_code  label         4         5         6  \\\n",
       "0      1.0       1.0        1.0    3.0  0.125901  0.077848 -0.112097   \n",
       "1      1.0       1.0        1.0    3.0 -0.033489  0.214405 -0.042801   \n",
       "2      1.0       1.0        1.0    3.0  0.078853  0.225161 -0.003258   \n",
       "3      1.0       1.0        1.0    4.0 -0.427013  0.124016  0.209373   \n",
       "4      1.0       1.0        1.0    4.0 -0.258621  0.147098  0.127442   \n",
       "\n",
       "          7         8         9    ...         1205      1206      1207  \\\n",
       "0 -0.011347  0.026207  0.056144    ...     0.827352 -2.067576  1.370951   \n",
       "1  0.036700  0.163193  0.102776    ...     0.959836 -1.320655  1.370951   \n",
       "2  0.106326  0.214892  0.254953    ...     1.985645  3.946246  1.370951   \n",
       "3 -0.285074  0.104775 -0.032565    ...     2.156472  4.686816  1.370951   \n",
       "4 -0.136984  0.028601 -0.016087    ...     2.010624  4.058232  1.370951   \n",
       "\n",
       "       1208      1209      1210      1211      1212      1213      1214  \n",
       "0  0.727249  0.049420  0.135441  0.055171  0.030166  0.091560  0.060249  \n",
       "1  0.544320  0.040074  0.131500  0.064495  0.057668  0.040334  0.029518  \n",
       "2  0.082254  0.040935  0.116699  0.070139  0.065564  0.061595  0.071664  \n",
       "3  0.393702  0.034751  0.106485  0.036636  0.047692  0.069517  0.017662  \n",
       "4  0.253397  0.035888  0.147006  0.043686  0.049197  0.026301  0.069618  \n",
       "\n",
       "[5 rows x 1215 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# location: 0 - N/A; 1 - pouch; 2 - pocket; 3 - hand\n",
    "# subj_code: 0 - amputee; 1 - healthy\n",
    "# label: 1 - slip; 2 - trip; 3 - right; 4 - left\n",
    "data_CF_df=data_CF_df.rename(columns = {0:'subj_id', 1:'location', 2:'subj_code', 3:'label'})\n",
    "\n",
    "# save data to file\n",
    "data_CF_df.to_csv('data_CF_10sec.csv')\n",
    "\n",
    "data_CF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_AF = eng.TrainingDataSetup([],[],1,1) #location -use all; subjID - use all; n jitter - 1; condition - amputees\n",
    "# data_AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1601, 1215)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1205</th>\n",
       "      <th>1206</th>\n",
       "      <th>1207</th>\n",
       "      <th>1208</th>\n",
       "      <th>1209</th>\n",
       "      <th>1210</th>\n",
       "      <th>1211</th>\n",
       "      <th>1212</th>\n",
       "      <th>1213</th>\n",
       "      <th>1214</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.152234</td>\n",
       "      <td>-0.431253</td>\n",
       "      <td>0.009568</td>\n",
       "      <td>-0.032451</td>\n",
       "      <td>-0.210494</td>\n",
       "      <td>-0.051874</td>\n",
       "      <td>...</td>\n",
       "      <td>2.077791</td>\n",
       "      <td>4.355384</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>-0.006816</td>\n",
       "      <td>0.026533</td>\n",
       "      <td>0.054728</td>\n",
       "      <td>0.062460</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.048826</td>\n",
       "      <td>0.044500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.013777</td>\n",
       "      <td>-0.165121</td>\n",
       "      <td>-0.049645</td>\n",
       "      <td>-0.011921</td>\n",
       "      <td>-0.164886</td>\n",
       "      <td>-0.049621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017431</td>\n",
       "      <td>-2.426575</td>\n",
       "      <td>2.321928</td>\n",
       "      <td>-0.725191</td>\n",
       "      <td>0.029039</td>\n",
       "      <td>0.082529</td>\n",
       "      <td>0.041031</td>\n",
       "      <td>0.061260</td>\n",
       "      <td>0.050184</td>\n",
       "      <td>0.049987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.018117</td>\n",
       "      <td>-0.158916</td>\n",
       "      <td>-0.049124</td>\n",
       "      <td>-0.016485</td>\n",
       "      <td>-0.166230</td>\n",
       "      <td>-0.047156</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156955</td>\n",
       "      <td>4.689407</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.468556</td>\n",
       "      <td>0.030050</td>\n",
       "      <td>0.084234</td>\n",
       "      <td>0.079322</td>\n",
       "      <td>0.072924</td>\n",
       "      <td>0.041299</td>\n",
       "      <td>0.044606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.011725</td>\n",
       "      <td>-0.167109</td>\n",
       "      <td>-0.050006</td>\n",
       "      <td>-0.015319</td>\n",
       "      <td>-0.166011</td>\n",
       "      <td>-0.051451</td>\n",
       "      <td>...</td>\n",
       "      <td>2.135102</td>\n",
       "      <td>4.672102</td>\n",
       "      <td>1.921928</td>\n",
       "      <td>0.636765</td>\n",
       "      <td>0.024369</td>\n",
       "      <td>0.039472</td>\n",
       "      <td>0.047402</td>\n",
       "      <td>0.027406</td>\n",
       "      <td>0.023649</td>\n",
       "      <td>0.079729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.014867</td>\n",
       "      <td>-0.171112</td>\n",
       "      <td>-0.048433</td>\n",
       "      <td>-0.019685</td>\n",
       "      <td>-0.167537</td>\n",
       "      <td>-0.050042</td>\n",
       "      <td>...</td>\n",
       "      <td>2.209490</td>\n",
       "      <td>4.910050</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>-0.151433</td>\n",
       "      <td>0.022384</td>\n",
       "      <td>0.038293</td>\n",
       "      <td>0.039684</td>\n",
       "      <td>0.028555</td>\n",
       "      <td>0.068377</td>\n",
       "      <td>0.033839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3         4         5         6         7         8     \\\n",
       "0   1.0   3.0   0.0   9.0  0.152234 -0.431253  0.009568 -0.032451 -0.210494   \n",
       "1   1.0   3.0   0.0   9.0 -0.013777 -0.165121 -0.049645 -0.011921 -0.164886   \n",
       "2   1.0   3.0   0.0   9.0 -0.018117 -0.158916 -0.049124 -0.016485 -0.166230   \n",
       "3   1.0   3.0   0.0   9.0 -0.011725 -0.167109 -0.050006 -0.015319 -0.166011   \n",
       "4   1.0   3.0   0.0   9.0 -0.014867 -0.171112 -0.048433 -0.019685 -0.167537   \n",
       "\n",
       "       9       ...         1205      1206      1207      1208      1209  \\\n",
       "0 -0.051874    ...     2.077791  4.355384  1.370951 -0.006816  0.026533   \n",
       "1 -0.049621    ...    -0.017431 -2.426575  2.321928 -0.725191  0.029039   \n",
       "2 -0.047156    ...     2.156955  4.689407  1.370951  0.468556  0.030050   \n",
       "3 -0.051451    ...     2.135102  4.672102  1.921928  0.636765  0.024369   \n",
       "4 -0.050042    ...     2.209490  4.910050  1.370951 -0.151433  0.022384   \n",
       "\n",
       "       1210      1211      1212      1213      1214  \n",
       "0  0.054728  0.062460  0.060200  0.048826  0.044500  \n",
       "1  0.082529  0.041031  0.061260  0.050184  0.049987  \n",
       "2  0.084234  0.079322  0.072924  0.041299  0.044606  \n",
       "3  0.039472  0.047402  0.027406  0.023649  0.079729  \n",
       "4  0.038293  0.039684  0.028555  0.068377  0.033839  \n",
       "\n",
       "[5 rows x 1215 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_AF_np = np.array(data_AF)\n",
    "data_AF_df = pd.DataFrame(data_AF_np)\n",
    "\n",
    "print(data_AF_df.shape)\n",
    "data_AF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj_id</th>\n",
       "      <th>location</th>\n",
       "      <th>subj_code</th>\n",
       "      <th>label</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1205</th>\n",
       "      <th>1206</th>\n",
       "      <th>1207</th>\n",
       "      <th>1208</th>\n",
       "      <th>1209</th>\n",
       "      <th>1210</th>\n",
       "      <th>1211</th>\n",
       "      <th>1212</th>\n",
       "      <th>1213</th>\n",
       "      <th>1214</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.152234</td>\n",
       "      <td>-0.431253</td>\n",
       "      <td>0.009568</td>\n",
       "      <td>-0.032451</td>\n",
       "      <td>-0.210494</td>\n",
       "      <td>-0.051874</td>\n",
       "      <td>...</td>\n",
       "      <td>2.077791</td>\n",
       "      <td>4.355384</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>-0.006816</td>\n",
       "      <td>0.026533</td>\n",
       "      <td>0.054728</td>\n",
       "      <td>0.062460</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.048826</td>\n",
       "      <td>0.044500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.013777</td>\n",
       "      <td>-0.165121</td>\n",
       "      <td>-0.049645</td>\n",
       "      <td>-0.011921</td>\n",
       "      <td>-0.164886</td>\n",
       "      <td>-0.049621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017431</td>\n",
       "      <td>-2.426575</td>\n",
       "      <td>2.321928</td>\n",
       "      <td>-0.725191</td>\n",
       "      <td>0.029039</td>\n",
       "      <td>0.082529</td>\n",
       "      <td>0.041031</td>\n",
       "      <td>0.061260</td>\n",
       "      <td>0.050184</td>\n",
       "      <td>0.049987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.018117</td>\n",
       "      <td>-0.158916</td>\n",
       "      <td>-0.049124</td>\n",
       "      <td>-0.016485</td>\n",
       "      <td>-0.166230</td>\n",
       "      <td>-0.047156</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156955</td>\n",
       "      <td>4.689407</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.468556</td>\n",
       "      <td>0.030050</td>\n",
       "      <td>0.084234</td>\n",
       "      <td>0.079322</td>\n",
       "      <td>0.072924</td>\n",
       "      <td>0.041299</td>\n",
       "      <td>0.044606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.011725</td>\n",
       "      <td>-0.167109</td>\n",
       "      <td>-0.050006</td>\n",
       "      <td>-0.015319</td>\n",
       "      <td>-0.166011</td>\n",
       "      <td>-0.051451</td>\n",
       "      <td>...</td>\n",
       "      <td>2.135102</td>\n",
       "      <td>4.672102</td>\n",
       "      <td>1.921928</td>\n",
       "      <td>0.636765</td>\n",
       "      <td>0.024369</td>\n",
       "      <td>0.039472</td>\n",
       "      <td>0.047402</td>\n",
       "      <td>0.027406</td>\n",
       "      <td>0.023649</td>\n",
       "      <td>0.079729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.014867</td>\n",
       "      <td>-0.171112</td>\n",
       "      <td>-0.048433</td>\n",
       "      <td>-0.019685</td>\n",
       "      <td>-0.167537</td>\n",
       "      <td>-0.050042</td>\n",
       "      <td>...</td>\n",
       "      <td>2.209490</td>\n",
       "      <td>4.910050</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>-0.151433</td>\n",
       "      <td>0.022384</td>\n",
       "      <td>0.038293</td>\n",
       "      <td>0.039684</td>\n",
       "      <td>0.028555</td>\n",
       "      <td>0.068377</td>\n",
       "      <td>0.033839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subj_id  location  subj_code  label         4         5         6  \\\n",
       "0      1.0       3.0        0.0    9.0  0.152234 -0.431253  0.009568   \n",
       "1      1.0       3.0        0.0    9.0 -0.013777 -0.165121 -0.049645   \n",
       "2      1.0       3.0        0.0    9.0 -0.018117 -0.158916 -0.049124   \n",
       "3      1.0       3.0        0.0    9.0 -0.011725 -0.167109 -0.050006   \n",
       "4      1.0       3.0        0.0    9.0 -0.014867 -0.171112 -0.048433   \n",
       "\n",
       "          7         8         9    ...         1205      1206      1207  \\\n",
       "0 -0.032451 -0.210494 -0.051874    ...     2.077791  4.355384  1.370951   \n",
       "1 -0.011921 -0.164886 -0.049621    ...    -0.017431 -2.426575  2.321928   \n",
       "2 -0.016485 -0.166230 -0.047156    ...     2.156955  4.689407  1.370951   \n",
       "3 -0.015319 -0.166011 -0.051451    ...     2.135102  4.672102  1.921928   \n",
       "4 -0.019685 -0.167537 -0.050042    ...     2.209490  4.910050  1.370951   \n",
       "\n",
       "       1208      1209      1210      1211      1212      1213      1214  \n",
       "0 -0.006816  0.026533  0.054728  0.062460  0.060200  0.048826  0.044500  \n",
       "1 -0.725191  0.029039  0.082529  0.041031  0.061260  0.050184  0.049987  \n",
       "2  0.468556  0.030050  0.084234  0.079322  0.072924  0.041299  0.044606  \n",
       "3  0.636765  0.024369  0.039472  0.047402  0.027406  0.023649  0.079729  \n",
       "4 -0.151433  0.022384  0.038293  0.039684  0.028555  0.068377  0.033839  \n",
       "\n",
       "[5 rows x 1215 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# location: 0 - N/A; 1 - pouch; 2 - pocket; 3 - hand\n",
    "# subj_code: 0 - amputee; 1 - healthy\n",
    "# label: 1 - slip; 2 - trip; 3 - right; 4 - left\n",
    "data_AF_df=data_AF_df.rename(columns = {0:'subj_id', 1:'location', 2:'subj_code', 3:'label'})\n",
    "\n",
    "# save data to file\n",
    "data_AF_df.to_csv('data_AF_10sec.csv')\n",
    "\n",
    "data_AF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj_id</th>\n",
       "      <th>location</th>\n",
       "      <th>subj_code</th>\n",
       "      <th>label</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1205</th>\n",
       "      <th>1206</th>\n",
       "      <th>1207</th>\n",
       "      <th>1208</th>\n",
       "      <th>1209</th>\n",
       "      <th>1210</th>\n",
       "      <th>1211</th>\n",
       "      <th>1212</th>\n",
       "      <th>1213</th>\n",
       "      <th>1214</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.279859</td>\n",
       "      <td>-0.451830</td>\n",
       "      <td>-0.122443</td>\n",
       "      <td>-0.014085</td>\n",
       "      <td>-0.168879</td>\n",
       "      <td>-0.048789</td>\n",
       "      <td>...</td>\n",
       "      <td>1.813089</td>\n",
       "      <td>3.258443</td>\n",
       "      <td>1.921928</td>\n",
       "      <td>0.317397</td>\n",
       "      <td>0.411891</td>\n",
       "      <td>0.951311</td>\n",
       "      <td>0.266262</td>\n",
       "      <td>0.182220</td>\n",
       "      <td>0.152750</td>\n",
       "      <td>0.223859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.381852</td>\n",
       "      <td>-0.552207</td>\n",
       "      <td>-0.178741</td>\n",
       "      <td>-0.008815</td>\n",
       "      <td>-0.160477</td>\n",
       "      <td>-0.048607</td>\n",
       "      <td>...</td>\n",
       "      <td>2.077591</td>\n",
       "      <td>4.353338</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.515837</td>\n",
       "      <td>0.029718</td>\n",
       "      <td>0.084029</td>\n",
       "      <td>0.023951</td>\n",
       "      <td>0.069011</td>\n",
       "      <td>0.018963</td>\n",
       "      <td>0.044672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.467158</td>\n",
       "      <td>-0.504300</td>\n",
       "      <td>-0.164920</td>\n",
       "      <td>-0.011402</td>\n",
       "      <td>-0.180339</td>\n",
       "      <td>-0.048061</td>\n",
       "      <td>...</td>\n",
       "      <td>1.356249</td>\n",
       "      <td>0.837893</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>-0.222621</td>\n",
       "      <td>0.498879</td>\n",
       "      <td>2.195032</td>\n",
       "      <td>0.441074</td>\n",
       "      <td>0.104369</td>\n",
       "      <td>0.141824</td>\n",
       "      <td>0.152883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.596201</td>\n",
       "      <td>0.219927</td>\n",
       "      <td>-0.109469</td>\n",
       "      <td>0.007806</td>\n",
       "      <td>0.228406</td>\n",
       "      <td>0.039248</td>\n",
       "      <td>...</td>\n",
       "      <td>2.041599</td>\n",
       "      <td>4.195732</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>0.756499</td>\n",
       "      <td>0.090853</td>\n",
       "      <td>0.213672</td>\n",
       "      <td>0.101003</td>\n",
       "      <td>0.101458</td>\n",
       "      <td>0.154579</td>\n",
       "      <td>0.176697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.412108</td>\n",
       "      <td>0.032183</td>\n",
       "      <td>-0.187147</td>\n",
       "      <td>-0.009107</td>\n",
       "      <td>-0.148790</td>\n",
       "      <td>-0.052507</td>\n",
       "      <td>...</td>\n",
       "      <td>2.235633</td>\n",
       "      <td>4.998432</td>\n",
       "      <td>0.721928</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>0.053629</td>\n",
       "      <td>0.115405</td>\n",
       "      <td>0.152212</td>\n",
       "      <td>0.076841</td>\n",
       "      <td>0.049582</td>\n",
       "      <td>0.080558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subj_id  location  subj_code  label         4         5         6  \\\n",
       "0      2.0       1.0        0.0    3.0 -0.279859 -0.451830 -0.122443   \n",
       "1      2.0       1.0        0.0    3.0 -0.381852 -0.552207 -0.178741   \n",
       "2      2.0       1.0        0.0    3.0 -0.467158 -0.504300 -0.164920   \n",
       "3      2.0       1.0        0.0    4.0  0.596201  0.219927 -0.109469   \n",
       "4      2.0       1.0        0.0    4.0  0.412108  0.032183 -0.187147   \n",
       "\n",
       "          7         8         9    ...         1205      1206      1207  \\\n",
       "0 -0.014085 -0.168879 -0.048789    ...     1.813089  3.258443  1.921928   \n",
       "1 -0.008815 -0.160477 -0.048607    ...     2.077591  4.353338  1.370951   \n",
       "2 -0.011402 -0.180339 -0.048061    ...     1.356249  0.837893  1.370951   \n",
       "3  0.007806  0.228406  0.039248    ...     2.041599  4.195732  1.370951   \n",
       "4 -0.009107 -0.148790 -0.052507    ...     2.235633  4.998432  0.721928   \n",
       "\n",
       "       1208      1209      1210      1211      1212      1213      1214  \n",
       "0  0.317397  0.411891  0.951311  0.266262  0.182220  0.152750  0.223859  \n",
       "1  0.515837  0.029718  0.084029  0.023951  0.069011  0.018963  0.044672  \n",
       "2 -0.222621  0.498879  2.195032  0.441074  0.104369  0.141824  0.152883  \n",
       "3  0.756499  0.090853  0.213672  0.101003  0.101458  0.154579  0.176697  \n",
       "4  0.481000  0.053629  0.115405  0.152212  0.076841  0.049582  0.080558  \n",
       "\n",
       "[5 rows x 1215 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from csv files\n",
    "data_CF_df = pd.read_csv('data_CF_10sec.csv', index_col=0)\n",
    "data_CF_df.head()\n",
    "\n",
    "data_AF_df = pd.read_csv('data_AF_10sec.csv', index_col=0)\n",
    "data_AF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "[ 11.  12.  13.  14.  15.  16.]\n"
     ]
    }
   ],
   "source": [
    "# make right and left latheral falls as 1 category\n",
    "data_CF_df.loc[data_CF_df.label == 4, ['label']] = 3\n",
    "data_AF_df.loc[data_AF_df.label == 4, ['label']] = 3\n",
    "\n",
    "# change amputee subjects ids\n",
    "max_id_CF = data_CF_df.subj_id.max()\n",
    "print(max_id_CF)\n",
    "data_AF_df.subj_id = data_AF_df.subj_id + max_id_CF\n",
    "print(data_AF_df.subj_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1091, 1215)\n",
      "(728, 1215)\n"
     ]
    }
   ],
   "source": [
    "# training and testing data split\n",
    "#data_train = data_CF_df.loc[data_CF_df['location'] == 1]\n",
    "#data_test = data_AF_df.loc[data_AF_df['location'] == 1]\n",
    "\n",
    "# location: 0 - N/A; 1 - pouch; 2 - pocket; 3 - hand\n",
    "# subj_code: 0 - amputee; 1 - healthy\n",
    "# label: 1 - slip; 2 - trip; 3 - right; 4 - left\n",
    "data_train = data_CF_df[(data_CF_df.location == 1) & ((data_CF_df.label>0) & (data_CF_df.label<=4))]\n",
    "data_test = data_AF_df[(data_AF_df.location == 1) & ((data_AF_df.label>0) & (data_AF_df.label<=4))]\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:  (113, 1215)\n",
      "\n",
      "All subjects ids:  [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      "Subjects for testing:  [  1.  10.]\n",
      "Subjects for training:  [ 2.  3.  4.  5.  6.  7.  8.  9.]\n",
      "Training data size:  (89, 1215)\n",
      "Testing data size:  (24, 1215)\n"
     ]
    }
   ],
   "source": [
    "# mix data\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data = data_CF_df\n",
    "full_data = data.append(data_AF_df)\n",
    "\n",
    "# location: 0 - N/A; 1 - pouch; 2 - pocket; 3 - hand\n",
    "# subj_code: 0 - amputee; 1 - healthy\n",
    "# label: 1 - slip; 2 - trip; 3 - right; 4 - left\n",
    "full_data = full_data[(full_data.location == 2) & ((full_data.label>0) & (full_data.label<4))]\n",
    "print(\"Data size: \", full_data.shape)\n",
    "print()\n",
    "\n",
    "subj_ids = np.sort(np.array(full_data.subj_id.unique()))\n",
    "print(\"All subjects ids: \",subj_ids)\n",
    "\n",
    "# first and last 2 subjects ids for testing\n",
    "test_subjects = subj_ids[np.r_[0:1, -1:0]]\n",
    "print(\"Subjects for testing: \",test_subjects)\n",
    "\n",
    "train_subjects = np.setdiff1d(subj_ids,test_subjects)\n",
    "print(\"Subjects for training: \",train_subjects)\n",
    "\n",
    "data_train = full_data.loc[full_data['subj_id'].isin(train_subjects)]\n",
    "data_test = full_data.loc[full_data['subj_id'].isin(test_subjects)]\n",
    "# group data by subject\n",
    "#groups = full_data.groupby('subj_id')\n",
    "#groups.head()\n",
    "\n",
    "#random.shuffle(groups)\n",
    "\n",
    "#for g, grp in groups:\n",
    "#    print (grp)\n",
    "\n",
    "#data = pd.DataFrame(groups)\n",
    "#print(data.shape)\n",
    "#data.head()\n",
    "\n",
    "#data_train, data_test = train_test_split(data, test_size=0.3)\n",
    "print(\"Training data size: \", data_train.shape)\n",
    "print(\"Testing data size: \",data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of features\n",
    "features = list(range(4,1215))\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   4    5    6    7    8    9   10   11   12   13   14   15   16   17   18\n",
      "   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33\n",
      "   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48\n",
      "   49   50   51   52   53   54   55   56   57   58   59   60   61   62   63\n",
      "   64   65   66   67   68   69   70   71   72   73   74   75   76   77   78\n",
      "   79   80   81   82   83   84   85   86   87   88   89   90   91   92   93\n",
      "   94   95   96   97   98   99  340  341  342  343  344  345  346  347  348\n",
      "  349  350  351  352  353  354  355  356  357  358  359  360  361  362  363\n",
      "  364  365  366  367  368  369  370  371  372  373  374  375  376  377  378\n",
      "  379  380  381  382  383  384  385  386  387  388  389  390  391  392  393\n",
      "  394  395  396  397  398  399  400  401  402  403  404  405  406  407  408\n",
      "  409  410  411  412  413  414  415  416  606  607  608  609  610  611  612\n",
      "  613  614  615  616  617  618  619  620  621  622  623  624  625  626  627\n",
      "  628  629  630  631  632  633  634  635  636  637  638  639  640  641  642\n",
      "  643  644  645  646  647  648  649  650  651  652  653  654  655  656  657\n",
      "  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672\n",
      "  673  674  675  676  677  678  679  680  681  682  683  684  685  686  687\n",
      "  688  689  690  691  692  693  694  695  696  697  698  699  700  701  942\n",
      "  943  944  945  946  947  948  949  950  951  952  953  954  955  956  957\n",
      "  958  959  960  961  962  963  964  965  966  967  968  969  970  971  972\n",
      "  973  974  975  976  977  978  979  980  981  982  983  984  985  986  987\n",
      "  988  989  990  991  992  993  994  995  996  997  998  999 1000 1001 1002\n",
      " 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017\n",
      " 1018]\n",
      "346\n"
     ]
    }
   ],
   "source": [
    "# handpicking features\n",
    "\n",
    "# feature groups:\n",
    "#       1 - Raw Signal Statistics\n",
    "#       2 - Raw Signal Correlation Coefficients\n",
    "#       3 - Raw Signal 5s FFT bins\n",
    "#       4 - Raw Signal 1s FFT bins\n",
    "#\t\t5 - Derivative Statistics\n",
    "#\t\t6 - Derivative 5s FFT bins\n",
    "#\t\t7 - Derivative 1s FFT bins\n",
    "#\t\t8 - Resultant Vector and Magnitude \n",
    "#\t\t9 - Angle Statistics (ArcTan)\n",
    "#\t\t10 - Entropies\n",
    "#\t\t11 - Raw Signal Cross Products\n",
    "#\t\t12 - Derivative Cross Products\n",
    "#\t\t13 - Raw Signal Statistics on 1s FFT bins\n",
    "#\t\t14 - Raw Signal Entropies on 1s FFT bins\n",
    "#\t\t15 - Raw Signal Statistics on 1s binned signal energy\n",
    "#\t\t16 - Derivative Statistics on 1s FFT bins\n",
    "#\t\t17 - Derivative Entropies on 1s FFT bins\n",
    "#\t\t18 - Barometer\n",
    "\n",
    "features_to_use = eng.getFeatureInds(matlab.logical([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]))\n",
    "\n",
    "features_to_use = np.array(features_to_use)\n",
    "features_to_use = [y for x in features_to_use for y in x] # flatten array\n",
    "features_to_use = np.array(features_to_use)\n",
    "\n",
    "features = features[features_to_use]\n",
    "print(features)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 150 (0.012130)\n",
      "2. feature 323 (0.009932)\n",
      "3. feature 213 (0.009832)\n",
      "4. feature 20 (0.009000)\n",
      "5. feature 162 (0.008694)\n",
      "6. feature 311 (0.008607)\n",
      "7. feature 19 (0.008175)\n",
      "8. feature 294 (0.007971)\n",
      "9. feature 200 (0.007834)\n",
      "10. feature 94 (0.007643)\n",
      "11. feature 303 (0.007534)\n",
      "12. feature 292 (0.007199)\n",
      "13. feature 43 (0.007007)\n",
      "14. feature 28 (0.006690)\n",
      "15. feature 16 (0.006497)\n",
      "16. feature 192 (0.006326)\n",
      "17. feature 178 (0.005866)\n",
      "18. feature 336 (0.005824)\n",
      "19. feature 133 (0.005729)\n",
      "20. feature 282 (0.005704)\n",
      "21. feature 175 (0.005634)\n",
      "22. feature 99 (0.005591)\n",
      "23. feature 88 (0.005512)\n",
      "24. feature 304 (0.005369)\n",
      "25. feature 216 (0.005355)\n",
      "26. feature 195 (0.005298)\n",
      "27. feature 15 (0.005248)\n",
      "28. feature 335 (0.005208)\n",
      "29. feature 31 (0.005201)\n",
      "30. feature 207 (0.005124)\n",
      "31. feature 91 (0.005024)\n",
      "32. feature 272 (0.004987)\n",
      "33. feature 191 (0.004972)\n",
      "34. feature 291 (0.004923)\n",
      "35. feature 290 (0.004897)\n",
      "36. feature 205 (0.004897)\n",
      "37. feature 151 (0.004797)\n",
      "38. feature 7 (0.004782)\n",
      "39. feature 269 (0.004766)\n",
      "40. feature 174 (0.004765)\n",
      "41. feature 102 (0.004763)\n",
      "42. feature 201 (0.004755)\n",
      "43. feature 118 (0.004704)\n",
      "44. feature 194 (0.004665)\n",
      "45. feature 42 (0.004625)\n",
      "46. feature 206 (0.004619)\n",
      "47. feature 305 (0.004618)\n",
      "48. feature 270 (0.004601)\n",
      "49. feature 289 (0.004452)\n",
      "50. feature 172 (0.004431)\n",
      "51. feature 0 (0.004401)\n",
      "52. feature 67 (0.004379)\n",
      "53. feature 198 (0.004303)\n",
      "54. feature 287 (0.004252)\n",
      "55. feature 208 (0.004230)\n",
      "56. feature 176 (0.004220)\n",
      "57. feature 85 (0.004186)\n",
      "58. feature 156 (0.004059)\n",
      "59. feature 214 (0.004043)\n",
      "60. feature 261 (0.004015)\n",
      "61. feature 186 (0.003982)\n",
      "62. feature 309 (0.003945)\n",
      "63. feature 202 (0.003941)\n",
      "64. feature 140 (0.003899)\n",
      "65. feature 342 (0.003887)\n",
      "66. feature 4 (0.003846)\n",
      "67. feature 92 (0.003843)\n",
      "68. feature 171 (0.003810)\n",
      "69. feature 165 (0.003797)\n",
      "70. feature 237 (0.003789)\n",
      "71. feature 131 (0.003745)\n",
      "72. feature 188 (0.003736)\n",
      "73. feature 82 (0.003726)\n",
      "74. feature 184 (0.003711)\n",
      "75. feature 114 (0.003702)\n",
      "76. feature 301 (0.003701)\n",
      "77. feature 273 (0.003693)\n",
      "78. feature 337 (0.003675)\n",
      "79. feature 66 (0.003658)\n",
      "80. feature 49 (0.003646)\n",
      "81. feature 100 (0.003646)\n",
      "82. feature 119 (0.003569)\n",
      "83. feature 300 (0.003567)\n",
      "84. feature 24 (0.003557)\n",
      "85. feature 157 (0.003549)\n",
      "86. feature 169 (0.003523)\n",
      "87. feature 210 (0.003494)\n",
      "88. feature 61 (0.003463)\n",
      "89. feature 116 (0.003452)\n",
      "90. feature 180 (0.003438)\n",
      "91. feature 1 (0.003420)\n",
      "92. feature 96 (0.003417)\n",
      "93. feature 73 (0.003406)\n",
      "94. feature 271 (0.003395)\n",
      "95. feature 83 (0.003392)\n",
      "96. feature 299 (0.003373)\n",
      "97. feature 155 (0.003373)\n",
      "98. feature 23 (0.003359)\n",
      "99. feature 189 (0.003355)\n",
      "100. feature 87 (0.003253)\n",
      "101. feature 298 (0.003250)\n",
      "102. feature 321 (0.003173)\n",
      "103. feature 187 (0.003171)\n",
      "104. feature 193 (0.003169)\n",
      "105. feature 173 (0.003148)\n",
      "106. feature 302 (0.003139)\n",
      "107. feature 168 (0.003117)\n",
      "108. feature 177 (0.003113)\n",
      "109. feature 268 (0.003107)\n",
      "110. feature 132 (0.003040)\n",
      "111. feature 307 (0.003039)\n",
      "112. feature 80 (0.003032)\n",
      "113. feature 86 (0.003025)\n",
      "114. feature 327 (0.003021)\n",
      "115. feature 145 (0.003008)\n",
      "116. feature 117 (0.003000)\n",
      "117. feature 69 (0.002997)\n",
      "118. feature 64 (0.002993)\n",
      "119. feature 14 (0.002991)\n",
      "120. feature 68 (0.002989)\n",
      "121. feature 163 (0.002982)\n",
      "122. feature 204 (0.002982)\n",
      "123. feature 115 (0.002955)\n",
      "124. feature 223 (0.002951)\n",
      "125. feature 159 (0.002945)\n",
      "126. feature 232 (0.002944)\n",
      "127. feature 153 (0.002930)\n",
      "128. feature 297 (0.002896)\n",
      "129. feature 308 (0.002881)\n",
      "130. feature 142 (0.002880)\n",
      "131. feature 170 (0.002869)\n",
      "132. feature 79 (0.002848)\n",
      "133. feature 306 (0.002843)\n",
      "134. feature 199 (0.002837)\n",
      "135. feature 54 (0.002817)\n",
      "136. feature 5 (0.002808)\n",
      "137. feature 319 (0.002804)\n",
      "138. feature 11 (0.002793)\n",
      "139. feature 149 (0.002792)\n",
      "140. feature 17 (0.002776)\n",
      "141. feature 2 (0.002773)\n",
      "142. feature 334 (0.002771)\n",
      "143. feature 97 (0.002756)\n",
      "144. feature 101 (0.002736)\n",
      "145. feature 183 (0.002704)\n",
      "146. feature 78 (0.002697)\n",
      "147. feature 57 (0.002697)\n",
      "148. feature 52 (0.002684)\n",
      "149. feature 3 (0.002681)\n",
      "150. feature 330 (0.002671)\n",
      "151. feature 332 (0.002658)\n",
      "152. feature 233 (0.002654)\n",
      "153. feature 166 (0.002649)\n",
      "154. feature 27 (0.002634)\n",
      "155. feature 18 (0.002616)\n",
      "156. feature 345 (0.002602)\n",
      "157. feature 130 (0.002596)\n",
      "158. feature 112 (0.002596)\n",
      "159. feature 252 (0.002580)\n",
      "160. feature 141 (0.002573)\n",
      "161. feature 34 (0.002554)\n",
      "162. feature 181 (0.002553)\n",
      "163. feature 146 (0.002541)\n",
      "164. feature 72 (0.002517)\n",
      "165. feature 22 (0.002517)\n",
      "166. feature 121 (0.002515)\n",
      "167. feature 60 (0.002515)\n",
      "168. feature 293 (0.002507)\n",
      "169. feature 196 (0.002501)\n",
      "170. feature 266 (0.002500)\n",
      "171. feature 283 (0.002496)\n",
      "172. feature 6 (0.002477)\n",
      "173. feature 134 (0.002468)\n",
      "174. feature 139 (0.002466)\n",
      "175. feature 286 (0.002441)\n",
      "176. feature 310 (0.002435)\n",
      "177. feature 90 (0.002432)\n",
      "178. feature 241 (0.002408)\n",
      "179. feature 89 (0.002391)\n",
      "180. feature 39 (0.002390)\n",
      "181. feature 164 (0.002389)\n",
      "182. feature 265 (0.002382)\n",
      "183. feature 98 (0.002379)\n",
      "184. feature 242 (0.002371)\n",
      "185. feature 260 (0.002368)\n",
      "186. feature 161 (0.002368)\n",
      "187. feature 343 (0.002360)\n",
      "188. feature 158 (0.002338)\n",
      "189. feature 160 (0.002327)\n",
      "190. feature 128 (0.002313)\n",
      "191. feature 81 (0.002312)\n",
      "192. feature 318 (0.002303)\n",
      "193. feature 147 (0.002299)\n",
      "194. feature 71 (0.002295)\n",
      "195. feature 215 (0.002291)\n",
      "196. feature 109 (0.002289)\n",
      "197. feature 9 (0.002283)\n",
      "198. feature 10 (0.002282)\n",
      "199. feature 37 (0.002277)\n",
      "200. feature 326 (0.002274)\n",
      "201. feature 328 (0.002262)\n",
      "202. feature 41 (0.002262)\n",
      "203. feature 230 (0.002257)\n",
      "204. feature 62 (0.002257)\n",
      "205. feature 219 (0.002248)\n",
      "206. feature 63 (0.002248)\n",
      "207. feature 74 (0.002233)\n",
      "208. feature 70 (0.002219)\n",
      "209. feature 104 (0.002217)\n",
      "210. feature 182 (0.002214)\n",
      "211. feature 138 (0.002212)\n",
      "212. feature 13 (0.002207)\n",
      "213. feature 258 (0.002205)\n",
      "214. feature 136 (0.002204)\n",
      "215. feature 218 (0.002203)\n",
      "216. feature 29 (0.002193)\n",
      "217. feature 26 (0.002179)\n",
      "218. feature 105 (0.002164)\n",
      "219. feature 93 (0.002155)\n",
      "220. feature 46 (0.002141)\n",
      "221. feature 36 (0.002132)\n",
      "222. feature 340 (0.002129)\n",
      "223. feature 25 (0.002126)\n",
      "224. feature 40 (0.002124)\n",
      "225. feature 126 (0.002123)\n",
      "226. feature 231 (0.002118)\n",
      "227. feature 33 (0.002112)\n",
      "228. feature 295 (0.002108)\n",
      "229. feature 224 (0.002106)\n",
      "230. feature 35 (0.002096)\n",
      "231. feature 277 (0.002092)\n",
      "232. feature 137 (0.002079)\n",
      "233. feature 262 (0.002066)\n",
      "234. feature 129 (0.002062)\n",
      "235. feature 45 (0.002045)\n",
      "236. feature 240 (0.002036)\n",
      "237. feature 185 (0.002033)\n",
      "238. feature 122 (0.002021)\n",
      "239. feature 154 (0.002013)\n",
      "240. feature 209 (0.002011)\n",
      "241. feature 59 (0.002008)\n",
      "242. feature 50 (0.002001)\n",
      "243. feature 325 (0.001993)\n",
      "244. feature 243 (0.001993)\n",
      "245. feature 167 (0.001991)\n",
      "246. feature 281 (0.001987)\n",
      "247. feature 329 (0.001979)\n",
      "248. feature 76 (0.001964)\n",
      "249. feature 8 (0.001957)\n",
      "250. feature 259 (0.001942)\n",
      "251. feature 222 (0.001941)\n",
      "252. feature 333 (0.001925)\n",
      "253. feature 120 (0.001911)\n",
      "254. feature 44 (0.001901)\n",
      "255. feature 125 (0.001893)\n",
      "256. feature 113 (0.001871)\n",
      "257. feature 234 (0.001869)\n",
      "258. feature 338 (0.001867)\n",
      "259. feature 197 (0.001862)\n",
      "260. feature 226 (0.001862)\n",
      "261. feature 253 (0.001847)\n",
      "262. feature 21 (0.001803)\n",
      "263. feature 179 (0.001783)\n",
      "264. feature 217 (0.001765)\n",
      "265. feature 106 (0.001751)\n",
      "266. feature 58 (0.001738)\n",
      "267. feature 312 (0.001731)\n",
      "268. feature 84 (0.001726)\n",
      "269. feature 245 (0.001692)\n",
      "270. feature 255 (0.001688)\n",
      "271. feature 95 (0.001687)\n",
      "272. feature 227 (0.001676)\n",
      "273. feature 314 (0.001671)\n",
      "274. feature 229 (0.001640)\n",
      "275. feature 316 (0.001638)\n",
      "276. feature 344 (0.001638)\n",
      "277. feature 190 (0.001634)\n",
      "278. feature 331 (0.001630)\n",
      "279. feature 339 (0.001628)\n",
      "280. feature 322 (0.001623)\n",
      "281. feature 152 (0.001619)\n",
      "282. feature 110 (0.001614)\n",
      "283. feature 127 (0.001608)\n",
      "284. feature 107 (0.001605)\n",
      "285. feature 53 (0.001595)\n",
      "286. feature 75 (0.001577)\n",
      "287. feature 212 (0.001572)\n",
      "288. feature 239 (0.001564)\n",
      "289. feature 203 (0.001504)\n",
      "290. feature 256 (0.001496)\n",
      "291. feature 244 (0.001494)\n",
      "292. feature 135 (0.001494)\n",
      "293. feature 313 (0.001471)\n",
      "294. feature 267 (0.001466)\n",
      "295. feature 51 (0.001466)\n",
      "296. feature 324 (0.001462)\n",
      "297. feature 32 (0.001456)\n",
      "298. feature 48 (0.001454)\n",
      "299. feature 38 (0.001446)\n",
      "300. feature 288 (0.001437)\n",
      "301. feature 236 (0.001437)\n",
      "302. feature 220 (0.001400)\n",
      "303. feature 111 (0.001393)\n",
      "304. feature 341 (0.001375)\n",
      "305. feature 221 (0.001375)\n",
      "306. feature 225 (0.001365)\n",
      "307. feature 284 (0.001359)\n",
      "308. feature 278 (0.001333)\n",
      "309. feature 296 (0.001329)\n",
      "310. feature 257 (0.001320)\n",
      "311. feature 264 (0.001317)\n",
      "312. feature 123 (0.001304)\n",
      "313. feature 65 (0.001302)\n",
      "314. feature 77 (0.001300)\n",
      "315. feature 47 (0.001288)\n",
      "316. feature 238 (0.001285)\n",
      "317. feature 211 (0.001274)\n",
      "318. feature 280 (0.001272)\n",
      "319. feature 249 (0.001265)\n",
      "320. feature 103 (0.001256)\n",
      "321. feature 55 (0.001244)\n",
      "322. feature 124 (0.001229)\n",
      "323. feature 317 (0.001226)\n",
      "324. feature 263 (0.001193)\n",
      "325. feature 254 (0.001187)\n",
      "326. feature 12 (0.001185)\n",
      "327. feature 320 (0.001183)\n",
      "328. feature 56 (0.001143)\n",
      "329. feature 144 (0.001136)\n",
      "330. feature 248 (0.001127)\n",
      "331. feature 143 (0.001118)\n",
      "332. feature 275 (0.001114)\n",
      "333. feature 250 (0.001091)\n",
      "334. feature 251 (0.001077)\n",
      "335. feature 276 (0.001068)\n",
      "336. feature 279 (0.001056)\n",
      "337. feature 235 (0.001044)\n",
      "338. feature 228 (0.000986)\n",
      "339. feature 315 (0.000870)\n",
      "340. feature 274 (0.000810)\n",
      "341. feature 285 (0.000808)\n",
      "342. feature 108 (0.000788)\n",
      "343. feature 148 (0.000713)\n",
      "344. feature 30 (0.000675)\n",
      "345. feature 247 (0.000670)\n",
      "346. feature 246 (0.000585)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16aafb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# features analysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "\n",
    "X = data_train[features]\n",
    "Y = data_train['label']\n",
    "\n",
    "forest.fit(X, Y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected number of features: 111\n",
      "1. feature 150 (1.000000)\n",
      "2. feature 323 (0.818830)\n",
      "3. feature 213 (0.810602)\n",
      "4. feature 20 (0.742006)\n",
      "5. feature 162 (0.716766)\n",
      "6. feature 311 (0.709607)\n",
      "7. feature 19 (0.673930)\n",
      "8. feature 294 (0.657148)\n",
      "9. feature 200 (0.645838)\n",
      "10. feature 94 (0.630089)\n",
      "11. feature 303 (0.621115)\n",
      "12. feature 292 (0.593498)\n",
      "13. feature 43 (0.577651)\n",
      "14. feature 28 (0.551514)\n",
      "15. feature 16 (0.535644)\n",
      "16. feature 192 (0.521540)\n",
      "17. feature 178 (0.483615)\n",
      "18. feature 336 (0.480110)\n",
      "19. feature 133 (0.472322)\n",
      "20. feature 282 (0.470270)\n",
      "21. feature 175 (0.464513)\n",
      "22. feature 99 (0.460924)\n",
      "23. feature 88 (0.454437)\n",
      "24. feature 304 (0.442632)\n",
      "25. feature 216 (0.441477)\n",
      "26. feature 195 (0.436801)\n",
      "27. feature 15 (0.432620)\n",
      "28. feature 335 (0.429323)\n",
      "29. feature 31 (0.428741)\n",
      "30. feature 207 (0.422398)\n",
      "31. feature 91 (0.414192)\n",
      "32. feature 272 (0.411134)\n",
      "33. feature 191 (0.409885)\n",
      "34. feature 291 (0.405839)\n",
      "35. feature 290 (0.403743)\n",
      "36. feature 205 (0.403732)\n",
      "37. feature 151 (0.395504)\n",
      "38. feature 7 (0.394269)\n",
      "39. feature 269 (0.392937)\n",
      "40. feature 174 (0.392821)\n",
      "41. feature 102 (0.392652)\n",
      "42. feature 201 (0.392039)\n",
      "43. feature 118 (0.387841)\n",
      "44. feature 194 (0.384612)\n",
      "45. feature 42 (0.381258)\n",
      "46. feature 206 (0.380827)\n",
      "47. feature 305 (0.380674)\n",
      "48. feature 270 (0.379306)\n",
      "49. feature 289 (0.367039)\n",
      "50. feature 172 (0.365299)\n",
      "51. feature 0 (0.362857)\n",
      "52. feature 67 (0.360981)\n",
      "53. feature 198 (0.354738)\n",
      "54. feature 287 (0.350543)\n",
      "55. feature 208 (0.348748)\n",
      "56. feature 176 (0.347879)\n",
      "57. feature 85 (0.345080)\n",
      "58. feature 156 (0.334655)\n",
      "59. feature 214 (0.333301)\n",
      "60. feature 261 (0.330979)\n",
      "61. feature 186 (0.328309)\n",
      "62. feature 309 (0.325234)\n",
      "63. feature 202 (0.324902)\n",
      "64. feature 140 (0.321462)\n",
      "65. feature 342 (0.320428)\n",
      "66. feature 4 (0.317069)\n",
      "67. feature 92 (0.316817)\n",
      "68. feature 171 (0.314068)\n",
      "69. feature 165 (0.313006)\n",
      "70. feature 237 (0.312386)\n",
      "71. feature 131 (0.308779)\n",
      "72. feature 188 (0.307974)\n",
      "73. feature 82 (0.307143)\n",
      "74. feature 184 (0.305943)\n",
      "75. feature 114 (0.305207)\n",
      "76. feature 301 (0.305085)\n",
      "77. feature 273 (0.304416)\n",
      "78. feature 337 (0.302933)\n",
      "79. feature 66 (0.301593)\n",
      "80. feature 49 (0.300577)\n",
      "81. feature 100 (0.300566)\n",
      "82. feature 119 (0.294216)\n",
      "83. feature 300 (0.294053)\n",
      "84. feature 24 (0.293210)\n",
      "85. feature 157 (0.292599)\n",
      "86. feature 169 (0.290434)\n",
      "87. feature 210 (0.288031)\n",
      "88. feature 61 (0.285484)\n",
      "89. feature 116 (0.284605)\n",
      "90. feature 180 (0.283419)\n",
      "91. feature 1 (0.281931)\n",
      "92. feature 96 (0.281704)\n",
      "93. feature 73 (0.280803)\n",
      "94. feature 271 (0.279885)\n",
      "95. feature 83 (0.279637)\n",
      "96. feature 299 (0.278069)\n",
      "97. feature 155 (0.278054)\n",
      "98. feature 23 (0.276915)\n",
      "99. feature 189 (0.276590)\n",
      "100. feature 87 (0.268205)\n",
      "101. feature 298 (0.267909)\n",
      "102. feature 321 (0.261599)\n",
      "103. feature 187 (0.261410)\n",
      "104. feature 193 (0.261241)\n",
      "105. feature 173 (0.259549)\n",
      "106. feature 302 (0.258760)\n",
      "107. feature 168 (0.256930)\n",
      "108. feature 177 (0.256668)\n",
      "109. feature 268 (0.256131)\n",
      "110. feature 132 (0.250632)\n",
      "111. feature 307 (0.250545)\n"
     ]
    }
   ],
   "source": [
    "# normalize feature importances\n",
    "importances = np.array(importances)\n",
    "importances /= importances.max()\n",
    "\n",
    "# select features > 25 % importance\n",
    "features = [i for i,j in zip(features,importances) if j > 0.25]\n",
    "\n",
    "X = data_train[features]\n",
    "Y = data_train['label']\n",
    "\n",
    "print (\"Selected number of features: %d\" % len(features))\n",
    "\n",
    "for f in range(len(features)):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics, svm, neighbors, linear_model, tree\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for  Decision Tree  set found on development set: {'min_samples_leaf': 1, 'max_depth': 20, 'min_samples_split': 1}\n",
      "\n",
      "Best estimator for  Decision Tree  model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=20,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=1, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "Best parameters for  Support Vector Machines  set found on development set: {'kernel': 'rbf', 'C': 10, 'gamma': 1e-05}\n",
      "\n",
      "Best estimator for  Support Vector Machines  model: SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1e-05, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Best parameters for  K-Nearest Neighbors  set found on development set: {'n_neighbors': 5}\n",
      "\n",
      "Best estimator for  K-Nearest Neighbors  model: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "Best parameters for  Logistic Regression  set found on development set: {'C': 1}\n",
      "\n",
      "Best estimator for  Logistic Regression  model: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Best parameters for  Random Forest  set found on development set: {'min_samples_leaf': 3, 'n_estimators': 5, 'max_depth': 50, 'min_samples_split': 3}\n",
      "\n",
      "Best estimator for  Random Forest  model: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=3, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "[[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=20,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=1, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), {'min_samples_leaf': 1, 'max_depth': 20, 'min_samples_split': 1}, 'Decision Tree'], [SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1e-05, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), {'kernel': 'rbf', 'C': 10, 'gamma': 1e-05}, 'Support Vector Machines'], [KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), {'n_neighbors': 5}, 'K-Nearest Neighbors'], [LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), {'C': 1}, 'Logistic Regression'], [RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=3, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), {'min_samples_leaf': 3, 'n_estimators': 5, 'max_depth': 50, 'min_samples_split': 3}, 'Random Forest']]\n"
     ]
    }
   ],
   "source": [
    "# Different models to try\n",
    "#       Model name ---------------------------------------------------------------------\n",
    "#      Parameters ------------------------------------------                           |\n",
    "#     Classifier -----------                               |                           |\n",
    "#                          |                               |                           |\n",
    "#                          v                               v                           |\n",
    "models = [[tree.DecisionTreeClassifier(), {'min_samples_split': [1, 3, 5, 7, 10, 20],# |\n",
    "                                           'min_samples_leaf': [1, 3, 5, 7, 10, 20],#  v\n",
    "                                           'max_depth': [5, 10, 20, 30, 40, 50, 100]}, \"Decision Tree\"]\n",
    "          ]\n",
    "\n",
    "models.append([svm.SVC(), {'kernel': ['rbf'], \n",
    "                           'gamma': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5], \n",
    "                           'C': [0.001, 0.01, 0.1, 1, 3, 5, 7, 10]}, \"Support Vector Machines\"])\n",
    "\n",
    "models.append([neighbors.KNeighborsClassifier(), {'n_neighbors': [1, 5, 10, 15, 20, 30, 50]}, \"K-Nearest Neighbors\"])\n",
    "\n",
    "models.append([linear_model.LogisticRegression(), {'C': [0.001, 0.01, 0.1, 1, 3, 5, 7, 10, 12, 15]}, \"Logistic Regression\"])\n",
    "\n",
    "models.append([ RandomForestClassifier(), {'n_estimators': [1, 5, 10, 20, 50],\n",
    "                                          'max_depth': [5, 10, 20, 30, 50],\n",
    "                                          'min_samples_leaf': [1, 3, 5, 7, 10, 20, 50],\n",
    "                                          'min_samples_split': [1, 3, 5, 7, 10]\n",
    "                                          }, \"Random Forest\"])\n",
    "\n",
    "models_with_best_params = []\n",
    "\n",
    "for model in models:\n",
    "    clf = GridSearchCV(model[0], model[1], cv=5)\n",
    "    clf.fit(X, Y)\n",
    "    best_params = clf.best_params_\n",
    "    best_estimator = clf.best_estimator_\n",
    "    \n",
    "    model_with_best_params = [best_estimator, best_params, model[2]]\n",
    "    \n",
    "    models_with_best_params.append(model_with_best_params)\n",
    "\n",
    "    print(\"Best parameters for \", model[2], \" set found on development set:\", best_params)\n",
    "    print()\n",
    "    print(\"Best estimator for \", model[2], \" model:\", best_estimator)\n",
    "# print(clf_tree.best_params_)\n",
    "# print()\n",
    "# print(\"Grid scores on development set:\")\n",
    "# print()\n",
    "# for params, mean_score, scores in clf_tree.grid_scores_:\n",
    "#     print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#           % (mean_score, scores.std() * 2, params))\n",
    "# print()\n",
    "\n",
    "print(models_with_best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy for Decision Tree model is 0.541666666667\n",
      "Classification report for classifier DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=20,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=1, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.33      0.17      0.22         6\n",
      "        2.0       0.38      0.83      0.53         6\n",
      "        3.0       0.88      0.58      0.70        12\n",
      "\n",
      "avg / total       0.62      0.54      0.54        24\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[1 5 0]\n",
      " [0 5 1]\n",
      " [2 3 7]]\n",
      "Prediction accuracy for Support Vector Machines model is 0.583333333333\n",
      "Classification report for classifier SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1e-05, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.33      0.17      0.22         6\n",
      "        2.0       0.75      0.50      0.60         6\n",
      "        3.0       0.59      0.83      0.69        12\n",
      "\n",
      "avg / total       0.56      0.58      0.55        24\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 1  1  4]\n",
      " [ 0  3  3]\n",
      " [ 2  0 10]]\n",
      "Prediction accuracy for K-Nearest Neighbors model is 0.458333333333\n",
      "Classification report for classifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.20      0.17      0.18         6\n",
      "        2.0       0.67      0.33      0.44         6\n",
      "        3.0       0.50      0.67      0.57        12\n",
      "\n",
      "avg / total       0.47      0.46      0.44        24\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[1 1 4]\n",
      " [0 2 4]\n",
      " [4 0 8]]\n",
      "Prediction accuracy for Logistic Regression model is 0.666666666667\n",
      "Classification report for classifier LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.33      0.17      0.22         6\n",
      "        2.0       0.71      0.83      0.77         6\n",
      "        3.0       0.71      0.83      0.77        12\n",
      "\n",
      "avg / total       0.62      0.67      0.63        24\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 1  2  3]\n",
      " [ 0  5  1]\n",
      " [ 2  0 10]]\n",
      "Prediction accuracy for Random Forest model is 0.5\n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=3, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.00      0.00      0.00         6\n",
      "        2.0       0.50      0.50      0.50         6\n",
      "        3.0       0.53      0.75      0.62        12\n",
      "\n",
      "avg / total       0.39      0.50      0.44        24\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[0 1 5]\n",
      " [0 3 3]\n",
      " [1 2 9]]\n"
     ]
    }
   ],
   "source": [
    "#fitting models to test_data\n",
    "for model in models_with_best_params:\n",
    "    classifier = model[0]\n",
    "    classifier.fit(X, Y)\n",
    "    score = classifier.score(data_test[features], data_test['label'])\n",
    "    print(\"Prediction accuracy for\", model[2], \"model is\", score)\n",
    "    expected = data_test['label']\n",
    "    predicted = classifier.predict(data_test[features])\n",
    "\n",
    "    print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "    print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
